# backend/app/api/endpoints/generate.py
import asyncio
import json
import traceback
import re
import aiofiles # FIX: Added missing import
from fastapi import APIRouter, HTTPException
from app.models import GenerateRequest
from app.agents.base_agent import AIAgent
from app.agents.ai_clients import get_client
from app.services import (
    file_handler,
    component_tester,
    e2e_tester,
    knowledge_base,
)
import os

# --- AGENT ROLE DEFINITIONS ---
PM_PROMPT = """You are a Project Manager AI. Your task is to take a user's website checklist and create a structured JSON project plan. The plan must be a JSON object with a 'tasks' array. Task types are 'component' or 'page'. You MUST use the Next.js App Router structure: pages are files like 'app/page.tsx' or 'app/contact/page.tsx'. Component tasks MUST come before page tasks."""

UI_DESIGNER_PROMPT = """You are a UI/UX Designer AI. Your task is to take a component description and create a JSON spec for its structure and props. For any text content like titles or labels, use descriptive placeholders in brackets, e.g., "[HERO_TITLE]". Your output must be a single JSON object."""

COPYWRITER_PROMPT = """You are an expert Copywriter AI. You will be given a component's design spec with placeholder text. Your task is to replace the placeholders with compelling, user-friendly copy. Your output must be a single JSON object containing only the finalized text content."""

FRONTEND_DEV_PROMPT = """You are an expert Frontend Developer specializing in Next.js, React, and TypeScript. Your task is to write code based on a JSON specification.
You must follow these strict rules:
1. All components are located in `src/components/`.
2. All pages are located in the `app/` directory.
3. When a page needs to import a component, you MUST use the full path alias starting from the root. For example: `import Header from '@/src/components/Header';`.
Your output must be a single JSON object with 'filename' (e.g., "src/components/Header.tsx") and 'content' keys."""

# FIX: Replaced the old prompt with a much more robust and specific one
DEBUGGER_PROMPT = """You are an expert Senior Debugger AI specializing in Next.js and React. Your task is to analyze an error log and the corresponding file content, then provide a complete, corrected version of the file.

Error types you will see:
1.  **Build Errors:** From the `next build` command. A common fix for "useState... only works in a Client Component" is to add `"use client";` as the very first line of the file.
2.  **Runtime Errors:** From a Playwright E2E test log. These often relate to null references (`Cannot read properties of null`), hydration mismatches, or invalid prop types.

Strict Rules:
1.  Your output MUST be a single JSON object.
2.  The JSON object must have a 'file_to_fix' key with the exact file path.
3.  The JSON object must have a 'content' key containing the ENTIRE, corrected file content as a single string.
"""

router = APIRouter()

# Configuration flags
MAX_BUILD_ATTEMPTS = 7

def parse_json_from_ai(ai_response: str) -> dict | None:
    """Parses a JSON object from a string, handling markdown code fences."""
    try:
        if "```json" in ai_response:
            ai_response = ai_response.split("```json")[1].split("```")[0]
        return json.loads(ai_response)
    except (json.JSONDecodeError, IndexError):
        print(f"Invalid JSON from AI: {ai_response[:200]}...")
        return None

async def ensure_nextjs_required_files(output_dir: str, checklist_data: dict):
    """Ensures all required Next.js files exist before attempting a build."""
    print("Ensuring required Next.js files exist...")
    
    # Create app/layout.tsx if it's missing
    layout_path = f"{output_dir}/app/layout.tsx"
    if not os.path.exists(layout_path):
        print("Creating missing app/layout.tsx...")
        site_title = checklist_data.get('branding', {}).get('siteName', 'Generated Website')
        site_description = "A website generated by Website Factory AI"
        
        layout_content = f'''import type {{ Metadata }} from 'next'
import './globals.css'

export const metadata: Metadata = {{
  title: '{site_title}',
  description: '{site_description}',
}}

export default function RootLayout({{
  children,
}}: {{
  children: React.ReactNode
}}) {{
  return (
    <html lang="en">
      <body className="antialiased">
        {{children}}
      </body>
    </html>
  )
}}'''
        await file_handler.write_file(output_dir, "app/layout.tsx", layout_content)
    
    # Create app/globals.css if it's missing
    globals_path = f"{output_dir}/app/globals.css"
    if not os.path.exists(globals_path):
        print("Creating missing app/globals.css...")
        globals_content = '''@tailwind base;
@tailwind components;
@tailwind utilities;'''
        await file_handler.write_file(output_dir, "app/globals.css", globals_content)
    
    print("Required Next.js files validated.")


async def build_component_simple(task: dict, output_dir: str, agents: dict) -> bool:
    """Builds a component without individual testing."""
    task_name = task.get("name")
    print(f"\\nBuilding Component: {task_name}")
    
    try:
        spec_response = agents['ui'].execute_task({"component": task_name, "props": task.get("details", {})})
        design_spec = parse_json_from_ai(spec_response)
        if not design_spec: return False

        copy_response = agents['copy'].execute_task({"design_spec": design_spec})
        final_copy = parse_json_from_ai(copy_response)
        if final_copy and isinstance(design_spec, dict):
            design_spec.setdefault('props', {}).update(final_copy)

        code_response = agents['dev'].execute_task({"componentSpec": design_spec})
        component_file = parse_json_from_ai(code_response)
        
        if component_file and component_file.get("filename") and component_file.get("content"):
            await file_handler.write_file(output_dir, component_file["filename"], component_file["content"])
            print(f"Created {task_name}")
            return True
        return False
    except Exception as e:
        print(f"Failed to create {task_name}: {e}")
        return False

async def build_page_simple(task: dict, output_dir: str, agents: dict) -> bool:
    """Builds a page."""
    page_name = task.get("name")
    print(f"\\nBuilding Page: {page_name}")
    
    try:
        page_response = agents['dev'].execute_task({
            "task": "create_page",
            "pageName": page_name,
            "details": task.get("details", {}),
        })
        page_file = parse_json_from_ai(page_response)
        
        if page_file and page_file.get("filename") and page_file.get("content"):
            await file_handler.write_file(output_dir, page_file["filename"], page_file["content"])
            print(f"Created page: {page_name}")
            return True
        return False
    except Exception as e:
        print(f"Failed to create page {page_name}: {e}")
        return False

# FEATURE: Enhanced E2E test generation to catch runtime errors
async def generate_simple_e2e_tests(output_dir: str, checklist_data: dict) -> dict:
    """Generates a Playwright test that checks for browser console errors."""
    import os
    pages_to_test = ["/"]
    for page in checklist_data.get("pages", []):
        if page.get("path") and page["path"] != "/":
            pages_to_test.append(page["path"])

    pages_list_json = json.dumps(pages_to_test)
    
    e2e_content = f"""import {{ test, expect }} from '@playwright/test';

test.describe('Website Health Checks', () => {{
  const pages = {pages_list_json};

  test('all pages should load without console errors', async ({{ page }}) => {{
    for (const pagePath of pages) {{
      const errors = [];
      page.on('console', msg => {{
        if (msg.type() === 'error') {{
          console.log(`RUNTIME ERROR on ${{pagePath}}: ${{msg.text()}}`);
          errors.push(msg.text());
        }}
      }});

      await page.goto(pagePath);
      await page.waitForTimeout(500);

      expect(errors).toEqual([]);
    }}
  }});
}});"""
    
    return {"filename": "tests/e2e.spec.ts", "content": e2e_content}


async def fix_build_error(error_log: str, output_dir: str, agents: dict) -> bool:
    """
    Attempts to fix a build error by first checking the knowledge base for a known solution,
    then attempting common fixes, and finally resorting to an AI-powered fix.
    """
    signature = knowledge_base.create_error_signature(error_log)
    
    # 1. Check for a known solution in the database first
    if signature:
        known_solution = knowledge_base.find_known_solution(signature)
        if known_solution:
            solution_data = json.loads(known_solution)
            if solution_data.get("action") == "reset_node_modules":
                print("✅ Applying known solution: Resetting node modules...")
                await component_tester.reset_node_modules(output_dir)
                knowledge_base.mark_solution_successful(signature)
                return True

    # 2. If no known solution, try the hardcoded dependency reset for the specific error
    if "Cannot find module" in error_log and "server/require-hook" in error_log:
        print("Applying hardcoded fix: Resetting dependencies for module resolution...")
        success = await component_tester.reset_node_modules(output_dir)
        if success:
            # Save the successful action to the knowledge base for next time
            action_taken = {"action": "reset_node_modules"}
            knowledge_base.save_incident(
                signature=signature,
                log=error_log,
                prompt={"fix_type": "hardcoded"},
                patch_or_action=action_taken,
                agent="SystemRule",
                attempts=1
            )
        return success

    # 3. If it's a different error, fall back to the AI debugger
    if "Failed to compile." in error_log or "Type error" in error_log or "Playwright" in error_log:
        try:
            # --- START: NEW CONTEXT-GATHERING LOGIC ---

            # 1. Read the ENTIRE codebase from the output directory
            full_codebase = await file_handler.read_all_code_files(output_dir)

            # 2. Find the specific file with the error to tell the AI where to focus
            match = re.search(r"\./(app/.*?\.tsx|src/.*?\.tsx)", error_log)
            file_with_error_path = match.group(1) if match else "Unknown"
            
            # --- END: NEW CONTEXT-GATHERING LOGIC ---
            
            # 3. Pass the full context to the debugger
            debugger_response = agents["debugger"].execute_task({
                "error_log": error_log,
                "file_with_error": file_with_error_path,
                "codebase": full_codebase  # Pass all the code instead of just one file
            })
            
            fix_data = parse_json_from_ai(debugger_response)
            if fix_data and "file_to_fix" in fix_data and "content" in fix_data:
                await file_handler.write_file(output_dir, fix_data["file_to_fix"], fix_data["content"])
                print(f"Applied AI fix to {fix_data['file_to_fix']}")
                return True

        except Exception as e:
            print(f"AI fix attempt failed: {e}")
            traceback.print_exc()

    return False

@router.post("/generate")
async def generate_website(request: GenerateRequest):
    session_id, output_dir = "", ""
    final_status = "FAILED"
    
    try:
        checklist_data = request.checklist.dict()
        session_id = knowledge_base.create_session(checklist_data)
        
        output_dir = file_handler.create_output_dir()
        file_handler.setup_scaffold(output_dir, checklist_data)
        await component_tester.install_dependencies(output_dir)
        
        ai_client = get_client()
        agents = {
            'pm': AIAgent(knowledge_base.get_active_prompt("PM") or PM_PROMPT, ai_client),
            'ui': AIAgent(knowledge_base.get_active_prompt("UI_DESIGNER") or UI_DESIGNER_PROMPT, ai_client),
            'copy': AIAgent(knowledge_base.get_active_prompt("COPYWRITER") or COPYWRITER_PROMPT, ai_client),
            'dev': AIAgent(knowledge_base.get_active_prompt("FRONTEND_DEV") or FRONTEND_DEV_PROMPT, ai_client),
            'debugger': AIAgent(knowledge_base.get_active_prompt("DEBUGGER") or DEBUGGER_PROMPT, ai_client)
        }
        
        plan_response = agents['pm'].execute_task({"checklist": checklist_data})
        project_plan = parse_json_from_ai(plan_response)
        if not project_plan or "tasks" not in project_plan:
            raise ValueError("PM failed to create a valid project plan")
        
        # Build components and pages
        for task in project_plan.get("tasks", []):
            if task.get("type") == "component":
                await build_component_simple(task, output_dir, agents)
            elif task.get("type") == "page":
                await build_page_simple(task, output_dir, agents)
        
        await ensure_nextjs_required_files(output_dir, checklist_data)
        
        # Build validation with retry logic
        build_successful = False
        for attempt in range(1, MAX_BUILD_ATTEMPTS + 1):
            print(f"\\nBuild Attempt {attempt}/{MAX_BUILD_ATTEMPTS}")
            build_ok, build_log = await e2e_tester.run_command_stream("npm run build", cwd=output_dir)
            
            if build_ok:
                print("✅ Build successful!")
                build_successful = True
                break
            else:
                print(f"Build failed:\\n{build_log[:1000]}...")
                if attempt < MAX_BUILD_ATTEMPTS:
                    if not await fix_build_error(build_log, output_dir, agents):
                        print("Could not auto-fix build error. Aborting build attempts.")
                        break
                    print("Applied fix, retrying build...")
                
        if not build_successful:
            raise Exception("Website generated but failed to build after multiple attempts.")
        
        # FEATURE: New runtime-fix loop using E2E tests
        e2e_tests_passed = False
        for attempt in range(1, MAX_BUILD_ATTEMPTS + 1):
            print(f"\\nE2E Test & Runtime Check Attempt {attempt}/{MAX_BUILD_ATTEMPTS}")
            
            e2e_code = await generate_simple_e2e_tests(output_dir, checklist_data)
            await file_handler.write_file(output_dir, e2e_code["filename"], e2e_code["content"])
            e2e_ok, e2e_log = await e2e_tester.execute_playwright_tests(output_dir)

            if e2e_ok:
                print("✅ E2E tests passed with no runtime errors!")
                e2e_tests_passed = True
                break
            else:
                print(f"E2E tests failed with runtime errors:\\n{e2e_log[:1000]}...")
                if attempt < MAX_BUILD_ATTEMPTS:
                    if await fix_build_error(e2e_log, output_dir, agents):
                        print("Applied runtime fix, rebuilding and re-testing...")
                        build_ok, build_log = await e2e_tester.run_command_stream("npm run build", cwd=output_dir)
                        if not build_ok:
                            print(f"Re-build failed after runtime fix attempt. Aborting. Log:\\n{build_log}")
                            break
                    else:
                        print("Could not auto-fix runtime error. Aborting test attempts.")
                        break
        
        final_status = "SUCCESS" if e2e_tests_passed else "FAILED_RUNTIME_TESTS"
        return {"status": final_status, "outputPath": output_dir}
        
    except Exception as e:
        traceback.print_exc()
        final_status = "FAILED"
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if session_id:
            knowledge_base.update_session_status(session_id, final_status, output_dir)